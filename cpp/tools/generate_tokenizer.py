from string import ascii_letters, digits


label_set_exclusion_set = set({
    "[", "]", "++", "--", "-=", "+=", "!=", "(", ")", "<shift<",
    ">shift>", "/=", "*=", "==", "<=", ">=", "^=", "{", "}", ":",
    "-", "+", "/", "^", "*", "%", "!", "<", ">", "\\n", "\\t", ",",
    ".",
    *digits
})

independant_tokens:list[str | tuple[str, list[str]]] = [
    "[", "]", "++", "--", "-=", "+=", "!=", "and", "or", "(", ")",
    "bit_or", "bit_and", "bit_not", "bit_xor", "<shift<", ">shift>",
    "/=", "*=", "==", "<=", ">=", "^=", "{",
    "}", ":", "import", "from", "as", "extension",
    "optional", "required", "\\t", "\\n", "fp10",
    "f16", "f32", "i8", "i16", "i32", "u8", "u16", "u32",
    "none", "bool", "lp", "mp", "hp", "vec2", "vec3", "vec4",
    "mat2x3", "mat3x2", "mat4x2", "mat2x4",
    "mat3x4", "mat4x3", "tex_1d", "tex_2d", "tex_3d", "tex_2d_array", "tex_2d_shadow",
    "tex_2d_shadow_array", "tex_cube", "tex_cube_shadow", "def", "uniform", "vertex",
    "in", "out", "func", "_",
    ",",
    ("mat2", [
        "x"
    ]),
    ("mat3", [
        "x"
    ]),
    ("mat4", [
        "x"
    ]),
    ("i", [
        "8", "1", "3", "n"
    ]),
    ("u", [
        "8", "n", "1", "3"
    ]),
    ("f", [
        "p", "1", "3", "u", "r"
    ]),
    ("-", [
        "-", "=", ">"
    ]),
    ("+", [
        "+", "="
    ]),
    ("^", [
        "="
    ]),
    ("*", [
        "="
    ]),
    ("/", [
        "="
    ]),
    ("%", [
        "="
    ]),
    ("!", [
        "="
    ]),
    ("<", [
        "=", "s"
    ]),
    (">", [
        "=", "s"
    ]),
    (".", [
        *digits
    ])
]

content = "// This file is auto generated by {working directory}/cpp/tools/generate_tokenizer.py\n#pragma once\n\n#include <set>\n#include <unordered_map>\n#include <string>\n\nusing std::set;\nusing std::unordered_map;\nusing std::string;\n\n"

content += "const unordered_map<string, set<char>> BUFFER_LOOK_AHEAD = {\n"

for tok in independant_tokens:
    if isinstance(tok, tuple):
        base, after = tok
        label_set = ""
        if base not in label_set_exclusion_set:
            label_set =  ascii_letters + digits + "_"
        content += f"    {{\"{base}\", {{{'\'' + '\', \''.join(after + [*label_set]) + '\''}}}}},\n"

    else:
        label_set = ""
        if tok not in label_set_exclusion_set:
            label_set = ascii_letters + digits + "_"
        content += f"    {{\"{tok}\", {{{'\'' + '\', \''.join([*label_set]) + '\'' if label_set != "" else ""}}}}},\n"

content += "};\n"

content += "const set<char> LABEL_SET = {\n"
content += "    '" + "', '".join(ascii_letters + "_" + digits) + "'\n"
content += "};\n"

content += "const set<char> DIGIT_SET = {\n"
content += "    '" + "', '".join("1234567890") + "'\n"
content += "};\n"

content += "const set<char> WHITESPACE_SET = {\n"
content += "    '" + "', '".join([" ","\\t","\\n"]) + "'\n"
content += "};\n"

with open("./cpp/include/tokenizer_constants.h", "w") as FP:
    FP.write(content)